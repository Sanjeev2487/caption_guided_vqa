# Codes for ACL 2019 paper (oral) Generating Question Relevant Captions to Aid VQA
## Pre-requirements
Pytorch >= 0.4.1 <br>
tqdm

## Preprocessing
(1) mkdir data && mkdir saved_models && mkdir vqa_models <br>
(2) Download data from [here](https://drive.google.com/drive/folders/1IXTsTudZtYLqmKzsXxIZbXfCnys_Izxr?usp=sharing) and put them under ``data`` folder <br>
(3) bash tools/download.sh <br>
(4) bash tools/preprocess.sh <br>

## Training
Our training process is splits into three stages. <br>
###(1) pretraining vqa models 
``CUDA_VISIBLE_DEVICES=0,1 python train_vqa_model.py --caption_dir None --learning_rate 0.0005 --joint_weight 1.0`` <br>
This command will save a vqa model under ``vqa_models`` folder. <br>
Alternatively, you can also download pretrained model from here and put it under ``vqa_models`` folder by: <br>
``wget -P vqa_models http://www.cs.utexas.edu/~jialinwu/dataset/vqa_model-best.pth``

###(2) training and extracting captions 
``CUDA_VISIBLE_DEVICES=0,1,2 python train_caption_model.py`` <br>
This command will save a vqa-caption model under ``saved_models`` folder named by the time you execute the command.

``CUDA_VISIBLE_DEVICES=0 python extract_caption_model.py --epoch 16`` <br>
This command will help you extract captions from the 16-th epochs using beam search. You can extract captions using different epoch numbers.
Then you merge the generated captions into one pkl file.

Alternatively, you can directly use captions I generated by <br>
``wget -P data http://www.cs.utexas.edu/~jialinwu/dataset/qid2caption_train.pkl`` <br>
``wget -P data http://www.cs.utexas.edu/~jialinwu/dataset/qid2caption_val.pkl`` <br>

###(3) training vqa models using generated captions 
``CUDA_VISIBLE_DEVICES=0,1 python finetune_vqa_model.py --caption_dir data/qid2caption --learning_rate 0.0005 --joint_weight 1.0`` <br>

I also release a final [vqa model](http://www.cs.utexas.edu/~jialinwu/dataset/vqa_model-best-final.pth) I trained using these codes.


