# Caption Guided Visual Question Answering

## Base Code Taken from [here](https://github.com/jialinwu17/generate_captions_for_vqa)
## Codes for ACL 2019 paper (oral) Generating Question Relevant Captions to Aid VQA
## Pre-requirements
Pytorch >= 0.4.1 <br>
tqdm

## Preprocessing
(1) mkdir data && mkdir saved_models && mkdir vqa_models <br>
(2) Download data from [here](https://drive.google.com/drive/folders/1IXTsTudZtYLqmKzsXxIZbXfCnys_Izxr?usp=sharing) and put them under ``data`` folder <br>
(3) bash tools/download.sh <br>
(4) bash tools/preprocess.sh <br>

## Training
Our training process is splits into three stages. <br>
### (1) pretraining vqa models 
``CUDA_VISIBLE_DEVICES=0,1 python train_vqa_model.py --caption_dir None --learning_rate 0.0005 --joint_weight 1.0`` <br>
This command will save a vqa model under ``vqa_models`` folder. <br>
Alternatively, you can also download pretrained model from here and put it under ``vqa_models`` folder by: <br>
``wget -P vqa_models http://www.cs.utexas.edu/~jialinwu/dataset/vqa_model-best.pth``

### (2) training and extracting captions 
``CUDA_VISIBLE_DEVICES=0,1,2 python train_caption_model.py`` <br>
This command will save a vqa-caption model under ``saved_models`` folder named by the time you execute the command.

``CUDA_VISIBLE_DEVICES=0 python extract_caption_model.py --epoch 16`` <br>
This command will help you extract captions from the 16-th epochs using beam search. You can extract captions using different epoch numbers. <br>

Alternatively, you can directly use captions I generated by <br>
``wget -P data http://www.cs.utexas.edu/~jialinwu/dataset/qid2caption_train.pkl`` <br>
``wget -P data http://www.cs.utexas.edu/~jialinwu/dataset/qid2caption_val.pkl`` <br>
I extracts 8 set of captions using different epoch numbers. When I merging them, I filtered out exactly the same captions for each QA pair.

### (3) training vqa models using generated captions 
``CUDA_VISIBLE_DEVICES=0,1 python finetune_vqa_model.py --caption_dir data/qid2caption --learning_rate 0.0005 --joint_weight 1.0`` <br>

I also release a final [vqa model](http://www.cs.utexas.edu/~jialinwu/dataset/vqa_model-best-final.pth) I trained using these codes.

## Citation
If you happen to find this work helpful, please consider cite the paper 
```@inproceedings{wu:acl19,
title={Generating Question Relevant Captions to Aid Visual Question Answering},
author={Jialin Wu and Zeyuan Hu and Raymond J. Mooney},
booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)},
month={August},
address={Florence, Italy},
url="http://www.cs.utexas.edu/users/ai-lab/pub-view.php?PubID=127759",
year={2019}
}

